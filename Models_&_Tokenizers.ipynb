{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuQwBhspa7z5ekFtgO5opA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qmeng222/transformers-for-NLP/blob/main/Models_%26_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NosqDtAWfTO-",
        "outputId": "62ca54e1-7774-4473-f6b8-1de1248e11f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers # install the Hugging Face Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer # import `AutoTokenizer` class to automatically load the appropriate tokenizer for a specific pre-trained model"
      ],
      "metadata": {
        "id": "ujU9MqC_fr8i"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bert-base-uncased\" # specify the pre-trained model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # load the appropriate tokenizer for the specified pre-trained model"
      ],
      "metadata": {
        "id": "YK4LpCQGgLVZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer # the tokenizer object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq7eCH6nkpZf",
        "outputId": "0876ef04-61fe-48bc-9e1b-73c50edce6a6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call the tokenizer object as a function with an input text:\n",
        "tokenizer(\"hello world\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOkmG4eslQkJ",
        "outputId": "1bf8a263-a323-4ab9-e8ff-cd93df2b972b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†The result is a dictionary containing different information, which is more than just a list of tokens.\n",
        "\n",
        " If you're preparing data for model input, using the callable `tokenizer` might be more appropriate."
      ],
      "metadata": {
        "id": "wMaW2APnwwps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods for the tokenizer object:"
      ],
      "metadata": {
        "id": "9gpA6Xoay9Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize an input text using the `tokenize` method of the `tokenizer`ï¼š\n",
        "tokens = tokenizer.tokenize(\"hello world\")\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6b99zlFmKRA",
        "outputId": "5f4fd7ba-941b-439f-e0dc-ad9499f6a379"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†The result is a list of tokens. Each element of the list corresponds to a tokenized unit.\n",
        "\n",
        "If you specifically need a list of tokens, then `tokenizer.tokenize` is suitable."
      ],
      "metadata": {
        "id": "hZs9Jb9RxTxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take a list of tokens & convert each token into its corresponding integer identifier (id):\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVu_l6vgyW8e",
        "outputId": "97787d12-b2e3-467d-af90-8f8230f506bf"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7592, 2088]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a list of integer identifiers (ids) & convert them into a list of tokens:\n",
        "tokenizer.convert_ids_to_tokens(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc0xNtsSy5yv",
        "outputId": "71a1510f-f713-4d8b-da0f-7d4fbec4d133"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†It is useful when you want to work with the individual tokens rather than a full decoded string."
      ],
      "metadata": {
        "id": "rJY3l8Vt3rJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# encode /decode:"
      ],
      "metadata": {
        "id": "HAlJ2sfUMr9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.encode(\"hello world\")\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTgeao1IMoqe",
        "outputId": "64482447-f559-45fc-caa1-1c05370483f1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 7592, 2088, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a list of integer identifiers (ids) & convert them into a single string:\n",
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KAF3k5Nhzn9w",
        "outputId": "433ddd5b-3968-4f6c-ad07-1a0a7883a7ce"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] hello world [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†It is useful for converting the model's output, which is typically in the form of integer IDs, back into human-readable text."
      ],
      "metadata": {
        "id": "0vMTiEZ33cLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call the tokenizer object as a function:\n",
        "model_inputs = tokenizer(\"hello world\")\n",
        "model_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr0nUbMC3gV3",
        "outputId": "6e46dd79-bcd2-4554-86ee-9b66d51372e0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple text inputs as a list:"
      ],
      "metadata": {
        "id": "STp_KoV44oU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "  \"I like cats.\",\n",
        "  \"Do you like cats too?\",\n",
        "]\n",
        "tokenizer(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQaCVElo4Te0",
        "outputId": "c43d06bc-887c-4c23-8185-082657859512"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 2066, 8870, 1012, 102], [101, 2079, 2017, 2066, 8870, 2205, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify a sequence of text into one or more predefined categories:"
      ],
      "metadata": {
        "id": "OPGskSY7-frN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification # import the class for sequence classification tasks"
      ],
      "metadata": {
        "id": "MP7v_TLf-Jz7"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the specified pre-trained model for sequence classification task:\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npmgWzar-c0D",
        "outputId": "7f90cf41-2811-492f-fed6-dfee0387b6bb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"hello world\", return_tensors='pt') #  return results in a format compatible with PyTorch tensors\n",
        "model_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_-Jf6NmFfyj",
        "outputId": "16a7a69e-fbdc-40f9-ecd9-b5672ae88239"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the default was to create a binary classifier!\n",
        "outputs = model(**model_inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQBK-LTwGRK5",
        "outputId": "0a0abaf2-827c-46ca-960c-68801b932b27"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6284, -0.0080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create another model, switching from binary to multi-class classification:"
      ],
      "metadata": {
        "id": "xRS5kTaxHO9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model for a classification task with 3 distinct labels:\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuhZOqoLGxQE",
        "outputId": "8b142d9d-b8ed-43d6-deb3-897d5b7753c0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**model_inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om21FsolIO7v",
        "outputId": "c54b9e4e-becd-432b-c345-17330a95e27b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2720, -0.0783, -0.1563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# raw scores or logits produced by the model before applying a final activation function (e.g., softmax in classification tasks):\n",
        "outputs.logits # treat `logits` as an attribute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsGex6AWIYqS",
        "outputId": "141b6695-baad-4e7a-a4d0-00ef2cbe6151"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2720, -0.0783, -0.1563]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs['logits'] # treat the outputs as dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5DOelqIIoIK",
        "outputId": "394ff415-c713-40d7-e244-a08dc3fe9400"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2720, -0.0783, -0.1563]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAjs6AYmIsd1",
        "outputId": "6bf2e389-bbf7-463e-9760-950e9d43ce43"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2720, -0.0783, -0.1563]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miZ8pW6oIyGG",
        "outputId": "98f99934-003a-47e9-8bfc-3b4952577539"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor -> np array:\n",
        "outputs.logits.detach().cpu().numpy()\n",
        "# .detach(): PyTorch method to create a new tensor that shares the same storage as the original tensor but with the computation history detached\n",
        "# .cpu(): iff the tensor is currently stored on a GPU, as NumPy operations don't operate on GPU tensors\n",
        "# .numpy(): convert the PyTorch tensor to a NumPy array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2rOjV5BI6ea",
        "outputId": "f1977f27-54ad-4b43-cca2-3a65dd5f872c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.27197528, -0.07832061, -0.15634893]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(data, padding=True, truncation=True, return_tensors='pt')\n",
        "model_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux-M5U3XKfSs",
        "outputId": "7254fe8e-d4d0-426c-885f-5a489d0c9d2d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
              "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs.input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQpJyBX6K1KD",
        "outputId": "f7b1488f-d9e6-490c-b54b-ef609bcb1e27"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
              "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_ZNNBKKpSf",
        "outputId": "b03f4fb8-ecfa-43c2-94d1-aac8260d9f4d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
              "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†Great, both ids have the same length as a result of padding."
      ],
      "metadata": {
        "id": "lOEogjx0RU67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7GNK8eTK40P",
        "outputId": "2af06937-b8c9-47e4-a44e-a838f71b6d1f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† 1 for real tokens, and 0 for padding tokens."
      ],
      "metadata": {
        "id": "VaLQwHyXRxO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pass tokenized data through the model:"
      ],
      "metadata": {
        "id": "d5qjSlIcSI7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**model_inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maQuD08NK_7U",
        "outputId": "247dbc64-6b60-4571-ea3f-59a67dfb31d8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4752, -0.1229, -0.1013],\n",
              "        [ 0.4151, -0.1148, -0.1456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† And we get numerical predictions!\n",
        "\n",
        "numerical predictions -- (post processing) --> human readable predictions"
      ],
      "metadata": {
        "id": "AN037JRPSVfS"
      }
    }
  ]
}