{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP4gkVnYmuknFVncnIVNmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qmeng222/transformers-for-NLP/blob/main/implement-transformers-from-scratch/decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges:\n",
        "*   Train from scratch (no fine-tuning)\n",
        "*   How does training and inference work"
      ],
      "metadata": {
        "id": "2JQKcnoO8gpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TOVsKAct8ew4"
      },
      "outputs": [],
      "source": [
        "import math # Python math module provides mathematical functions\n",
        "\n",
        "import torch # PyTorch library (a popular DL framework)\n",
        "import torch.nn as nn # (from PyTorch library) neural network module for building and training neural networks\n",
        "import torch.nn.functional as F # (within nn module) functional submodule provides functions (such as activation functions, loss functions, and other operations) that are applied element-wise to tensors\n",
        "from torch.utils.data import Dataset # Dataset class to customize datasets for training\n",
        "\n",
        "import numpy as np # NumPy library for numerical operations in Python\n",
        "import matplotlib.pyplot as plt # (from Matplotlib library) pyplot module for data visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len):\n",
        "    # assume d_v = d_k\n",
        "    # d_k: dimension of the key and value vectors\n",
        "    # d_model: dimension of the input vectors\n",
        "    # n_heads: number of attention heads\n",
        "    # max_len: max sequence length\n",
        "    super().__init__() # properly initialize the module and set up necessary attributes inherited from nn.Module\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # linear transformations for keys, queries, and values:\n",
        "    self.key = nn.Linear(d_model, d_k * n_heads)\n",
        "    self.query = nn.Linear(d_model, d_k * n_heads)\n",
        "    self.value = nn.Linear(d_model, d_k * n_heads)\n",
        "\n",
        "    # final linear layer is used for projection:\n",
        "    self.fc = nn.Linear(d_k * n_heads, d_model)\n",
        "\n",
        "    # causal mask (cm) as a lower triangular matrix to enforce causality in the self-attention mechanism:\n",
        "    cm = torch.tril(torch.ones(max_len, max_len)) # tril: \"lower triangular\" (all elements above the main diagonal are set to 0)\n",
        "    # register a buffer (tensor) as a persistent buffer of the module\n",
        "    self.register_buffer(\n",
        "        \"causal_mask\", # the name given to the buffer (to access the buffer later)\n",
        "        cm.view(1, 1, max_len, max_len) # create a view of the lower triangular matrix 'cm' with an additional two dimensions of size 1 at the beginning\n",
        "    )\n",
        "\n",
        "  def forward(self, q, k, v, pad_mask=None):\n",
        "    q = self.query(q) # N x T x (hd_k)\n",
        "    k = self.key(k)   # N x T x (hd_k)\n",
        "    v = self.value(v) # N x T x (hd_v)\n",
        "    N = q.shape[0]\n",
        "    T = q.shape[1]\n",
        "\n",
        "    # reshape: (N, T, h, d_k) -> (N, h, T, d_k)\n",
        "    # in order for matrix multiply to work properly\n",
        "    q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2) # (N, T, self.n_heads, self.d_k) -> (N, self.n_heads, T, self.d_k)\n",
        "    k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # compute attention weights\n",
        "    # (N, h, T, d_k) x (N, h, d_k, T) --> (N, h, T, T)\n",
        "    attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
        "\n",
        "    # if there's mask: each output can only pay attention to passed tokens\n",
        "    if pad_mask is not None:\n",
        "      # replace certain values in the 'attn_scores' tensor with float('-inf') based on the condition specified in the mask (pad_mask[:, None, None, :] == 0)\n",
        "      attn_scores = attn_scores.masked_fill(\n",
        "          pad_mask[:, None, None, :] == 0, float('-inf'))\n",
        "\n",
        "    # else if there's no mask:\n",
        "    attn_scores = attn_scores.masked_fill(\n",
        "        self.causal_mask[:, :, :T, :T] == 0, float('-inf')) # up to T (the seq len of the batch)\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # compute attention-weighted values\n",
        "    # (N, h, T, T) x (N, h, T, d_k) --> (N, h, T, d_k)\n",
        "    A = attn_weights @ v\n",
        "\n",
        "    # reshape it back before final linear layer\n",
        "    A = A.transpose(1, 2) # (N, T, h, d_k)\n",
        "    A = A.contiguous().view(N, T, self.d_k * self.n_heads) # (N, T, h*d_k)\n",
        "\n",
        "    # projection\n",
        "    return self.fc(A)"
      ],
      "metadata": {
        "id": "ZLp3rxvcqQ74"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(d_model)\n",
        "    self.ln2 = nn.LayerNorm(d_model)\n",
        "    self.mha = CausalSelfAttention(d_k, d_model, n_heads, max_len)\n",
        "    self.ann = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model * 4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(d_model * 4, d_model),\n",
        "        nn.Dropout(dropout_prob),\n",
        "    )\n",
        "    self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):\n",
        "    x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oLtghPLAXCr2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    exp_term = torch.arange(0, d_model, 2)\n",
        "    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(1, max_len, d_model)\n",
        "    pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "    pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.shape: N x T x D\n",
        "    x = x + self.pe[:, :x.size(1), :]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "KNt8vV6MYqki"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               max_len,\n",
        "               d_k,\n",
        "               d_model,\n",
        "               n_heads,\n",
        "               n_layers,\n",
        "               dropout_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
        "    transformer_blocks = [\n",
        "        TransformerBlock(\n",
        "            d_k,\n",
        "            d_model,\n",
        "            n_heads,\n",
        "            max_len,\n",
        "            dropout_prob) for _ in range(n_layers)]\n",
        "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
        "    self.ln = nn.LayerNorm(d_model)\n",
        "    self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, pad_mask)\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x) # many-to-many\n",
        "    return x"
      ],
      "metadata": {
        "id": "CcyXT_PCaEmN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy test the decoder:\n",
        "model = Decoder(20_000, 1024, 16, 64, 4, 2, 0.1)"
      ],
      "metadata": {
        "id": "n-aRsPryaGJ8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMfJF_UsdNfY",
        "outputId": "7f8b860c-96ac-42be-9585-7aedab0bf98d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(20000, 64)\n",
              "  (pos_encoding): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): CausalSelfAttention(\n",
              "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
              "      )\n",
              "      (ann): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=64, out_features=20000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randint(0, 20_000, size=(8, 512))\n",
        "x_t = torch.tensor(x).to(device)"
      ],
      "metadata": {
        "id": "RSRgDICBdQ-D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without mask:\n",
        "y = model(x_t)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAog7-RudS2n",
        "outputId": "2845e104-a4bb-40fc-e1ca-f7db1bb2dd84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 20000])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👆 8 samples, with seq len of 512, and each prediction can belong to 20000 possible tokens."
      ],
      "metadata": {
        "id": "GWtr2PKwdsmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.ones((8, 512))\n",
        "mask[:, 256:] = 0\n",
        "mask_t = torch.tensor(mask).to(device)"
      ],
      "metadata": {
        "id": "hQmSwm2bdYp2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with mask:\n",
        "y = model(x_t, mask_t)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVM3XSqeeN9O",
        "outputId": "da97ff7f-e37f-40d4-eb55-bd0c8b1d057b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 20000])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}